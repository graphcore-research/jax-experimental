{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX on IPU: stateful linear regression\n",
    "\n",
    "Stateful computations in JAX on IPU, following the official tutorial: https://jax.readthedocs.io/en/latest/jax-101/07-state.html\n",
    "\n",
    "The original example will run as-is on the IPU. Nevertheless, to make full advantage of the IPU hardware and improve user experience, the following modifications were made:\n",
    "\n",
    "* `jax_platforms = \"cpu,ipu\"`: use CPU as the default platform for initialization of parameters and dataset.\n",
    "* `donate_argnums = (0,)` in `update`: buffer donation to keep model parameters on the IPU SRAM between iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install experimental JAX for IPUs\n",
    "import sys\n",
    "!{sys.executable} -m pip uninstall -q -y jax jaxlib\n",
    "!{sys.executable} -m pip install -q https://github.com/graphcore-research/jax-experimental/releases/latest/download/jaxlib-0.3.15-cp38-none-manylinux2014_x86_64.whl\n",
    "!{sys.executable} -m pip install -q https://github.com/graphcore-research/jax-experimental/releases/latest/download/jax-0.3.16-py3-none-any.whl\n",
    "!{sys.executable} -m pip install -q matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.config import config\n",
    "\n",
    "# Use CPU as default backend for initialization.\n",
    "config.FLAGS.jax_platforms = \"cpu,ipu\"\n",
    "# Uncomment to use IPU model emulator.\n",
    "# config.FLAGS.jax_ipu_use_model = True\n",
    "# config.FLAGS.jax_ipu_model_num_tiles = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "print(\"IPU devices:\", jax.devices(\"ipu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "class Params(NamedTuple):\n",
    "  \"\"\"Linear regression state.\"\"\"\n",
    "  weight: jnp.ndarray\n",
    "  bias: jnp.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(rng) -> Params:\n",
    "  \"\"\"Returns the initial model params.\"\"\"\n",
    "  weights_key, bias_key = jax.random.split(rng)\n",
    "  weight = jax.random.normal(weights_key, ())\n",
    "  bias = jax.random.normal(bias_key, ())\n",
    "  return Params(weight, bias)\n",
    "\n",
    "\n",
    "def loss(params: Params, x: jnp.ndarray, y: jnp.ndarray) -> jnp.ndarray:\n",
    "  \"\"\"Computes the least squares error of the model's predictions on x against y.\"\"\"\n",
    "  pred = params.weight * x + params.bias\n",
    "  return jnp.mean((pred - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "# Explicit jitting for IPU backend.\n",
    "# Donate `params`` to keep variables on IPU SRAM. \n",
    "@partial(jax.jit, backend=\"ipu\", donate_argnums=(0,))\n",
    "def update(params: Params, x: jnp.ndarray, y: jnp.ndarray) -> Params:\n",
    "  \"\"\"Performs one SGD update step on params using the given data.\"\"\"\n",
    "  grad = jax.grad(loss)(params, x, y)\n",
    "\n",
    "  # If we were using Adam or another stateful optimizer,\n",
    "  # we would also do something like\n",
    "  # ```\n",
    "  # updates, new_optimizer_state = optimizer(grad, optimizer_state)\n",
    "  # ```\n",
    "  # and then use `updates` instead of `grad` to actually update the params.\n",
    "  # (And we'd include `new_optimizer_state` in the output, naturally.)\n",
    "\n",
    "  new_params = jax.tree_map(\n",
    "      lambda param, g: param - g * LEARNING_RATE, params, grad)\n",
    "\n",
    "  return new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "# Generate true data from y = w*x + b + noise\n",
    "true_w, true_b = 2, -1\n",
    "x_rng, noise_rng = jax.random.split(rng)\n",
    "xs = jax.random.normal(x_rng, (128, 1))\n",
    "noise = jax.random.normal(noise_rng, (128, 1)) * 0.5\n",
    "ys = xs * true_w + true_b + noise\n",
    "\n",
    "# Fit regression\n",
    "params = init(rng)\n",
    "for _ in range(1000):\n",
    "  params = update(params, xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "params = jax.device_get(params)\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "plt.plot(xs, params.weight * xs + params.bias, c='red', label='Model Prediction')\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
